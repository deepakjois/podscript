package ytt

import (
	"bufio"
	"context"
	"errors"
	"fmt"
	"net/http"
	"os"
	"path"
	"strings"
	"time"

	"github.com/cenkalti/backoff/v4"
	"github.com/kkdai/youtube/v2"
	"github.com/liushuangls/go-anthropic/v2"
	"github.com/sashabaranov/go-openai"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"github.com/thediveo/enumflag/v2"
)

const (
	userPrompt = "Your task is to clean up the following podcast transcription snippet and generate a clean version. Do not remove our modify anything. Output the most accurate transcription of the text"

	systemPrompt = "You are a top tier podcast transcriptionist skilled in grammer, spelling and punctuation. You specialize in taking rough transcripts and cleaning and formatting them with full accuracy. Use the following portion of the transcript from the beginning for more context. Only output the transcribed text"
)

type Model enumflag.Flag

// Enumeration of allowed ColorMode values.
const (
	ModelChatGPT Model = iota
	ModelClaude
)

// Defines the textual representations for the ColorMode values.
var modelMap = map[Model][]string{
	Model(ModelChatGPT): {"chatgpt"},
	Model(ModelClaude):  {"claude"},
}

func callChatGPTAPIWithBackoff(client *openai.Client, contextTxt, text string) (string, error) {

	req := openai.ChatCompletionRequest{
		Model: openai.GPT4o,
		Messages: []openai.ChatCompletionMessage{
			{
				Role:    openai.ChatMessageRoleSystem,
				Content: systemPrompt + "\n\n" + contextTxt,
			},
			{
				Role:    openai.ChatMessageRoleUser,
				Content: userPrompt + "\n\n" + text,
			},
		},
	}

	backOff := backoff.NewExponentialBackOff()
	backOff.MaxElapsedTime = 10 * time.Minute

	var resp openai.ChatCompletionResponse

	err := backoff.Retry(func() (err error) {
		resp, err = client.CreateChatCompletion(context.Background(), req)
		if err != nil {
			// Check if the error is a 429 (Too Many Requests) error
			var openAIError *openai.APIError
			if errors.As(err, &openAIError) {
				if openAIError.HTTPStatusCode == http.StatusTooManyRequests {
					// This is a 429 error, so we'll retry
					fmt.Printf("%v\n", err)
					fmt.Println("Retrying…")
					return err
				}
			}
			// For any other error, we'll stop retrying
			return backoff.Permanent(err)
		}
		return nil
	}, backOff)

	if err != nil {
		return "", err
	}
	if len(resp.Choices) == 0 {
		return "", fmt.Errorf("no choices returned from API")
	}

	// TODO: Log this as debug output
	// fmt.Printf("Usage: %+v\n", resp.Usage)
	return resp.Choices[0].Message.Content, nil
}

func callClaudeAPIWithBackoff(client *anthropic.Client, contextTxt, text string) (string, error) {
	req := &anthropic.MessagesRequest{
		Model:  anthropic.ModelClaude3Dot5Sonnet20240620,
		System: systemPrompt + "\n\n" + contextTxt,
		Messages: []anthropic.Message{
			anthropic.NewUserTextMessage(userPrompt + "\n\n" + text),
		},
		MaxTokens: 8192,
	}

	backOff := backoff.NewExponentialBackOff()
	backOff.MaxElapsedTime = 10 * time.Minute

	var resp anthropic.MessagesResponse

	err := backoff.Retry(func() (err error) {
		resp, err = client.CreateMessages(context.Background(), *req)
		if err != nil {
			var anthropicAPIError *anthropic.APIError
			if errors.As(err, &anthropicAPIError) {
				if anthropicAPIError.IsRateLimitErr() || anthropicAPIError.IsOverloadedErr() {
					fmt.Printf("%v\n", err)
					fmt.Println("Retrying…")
					return err
				}
			}
			// For any other error, we'll stop retrying
			return backoff.Permanent(err)
		}
		return nil
	}, backOff)

	if err != nil {
		return "", err
	}

	// TODO: Log this as debug output
	fmt.Printf("Usage: %+v\n", resp.Usage)
	return resp.GetFirstContentText(), nil
}

func chunkTranscript(transcript string, maxWordsPerChunk int) []string {
	// Split the transcript into chunks
	var chunks []string
	scanner := bufio.NewScanner(strings.NewReader(transcript))
	scanner.Split(bufio.ScanWords)

	var chunkBuilder strings.Builder
	wordCount := 0

	for scanner.Scan() {
		word := scanner.Text()
		chunkBuilder.WriteString(word + " ")
		wordCount++
		if wordCount >= maxWordsPerChunk {
			chunks = append(chunks, chunkBuilder.String())
			chunkBuilder.Reset()
			wordCount = 0
		}
	}
	if chunkBuilder.Len() > 0 {
		chunks = append(chunks, chunkBuilder.String())
	}
	return chunks

}

var Command = &cobra.Command{
	Use:   "ytt <youtube_url>",
	Short: "Generate cleaned up transcript from YouTube autogenerated captions using ChatGPT",
	Args:  cobra.ExactArgs(1),
	RunE: func(cmd *cobra.Command, args []string) error {
		raw, _ := cmd.Flags().GetBool("raw")
		model := cmd.Flags().Lookup("model").Value.String()

		openaiApiKey := viper.GetString("openai_api_key")
		if model == "chatgpt" && openaiApiKey == "" && !raw {
			return errors.New("OpenAI API key not found. Please run 'podscript configure' or set the PODSCRIPT_OPENAI_API_KEY environment variable")
		}

		anthropicApiKey := viper.GetString("anthropic_api_key")
		if model == "claude" && anthropicApiKey == "" && !raw {
			return errors.New("Anthropic API key not found. Please run 'podscript configure' or set the PODSCRIPT_ANTHROPIC_API_KEY environment variable")
		}

		folder, _ := cmd.Flags().GetString("path")
		suffix, _ := cmd.Flags().GetString("suffix")
		if folder != "" {
			fi, err := os.Stat(folder)
			if err != nil || !fi.IsDir() {
				return fmt.Errorf("path not found: %s", folder)
			}
		}
		timestamp := time.Now().Format("2006-01-02-150405")
		var filenameSuffix string
		if suffix == "" {
			filenameSuffix = timestamp
		} else {
			filenameSuffix = fmt.Sprintf("%s_%s", timestamp, suffix)
		}

		// Extract Transcript
		youtubeClient := youtube.Client{}

		video, err := youtubeClient.GetVideo(args[0])
		if err != nil {
			return fmt.Errorf("failed to get video info: %w", err)
		}

		transcript, err := youtubeClient.GetTranscript(video, "en")
		if err != nil {
			return fmt.Errorf("failed to get transcript info: %w", err)
		}

		var transcriptTxt string
		for _, tr := range transcript {
			transcriptTxt += tr.Text + "\n"
		}

		rawTranscriptFilename := path.Join(folder, fmt.Sprintf("raw_transcript_%s.txt", filenameSuffix))
		if err = os.WriteFile(rawTranscriptFilename, []byte(transcriptTxt), 0644); err != nil {
			return fmt.Errorf("failed to write raw transcript: %w", err)
		}
		fmt.Printf("wrote raw autogenerated captions to %s\n", rawTranscriptFilename)

		// Stop if only raw transcript required
		if raw {
			return nil
		}

		var maxWordsPerChunk int
		if model == "chatgpt" {
			maxWordsPerChunk = 3000
		} else if model == "claude" {
			maxWordsPerChunk = 5000
		}
		// Chunk and Send to OpenAI
		chunks := chunkTranscript(transcriptTxt, maxWordsPerChunk)
		// First chunk used as context
		contextTxt := chunks[0][:3000]

		var (
			openAPIClient   *openai.Client
			claudeAPIClient *anthropic.Client
		)

		if model == "chatgpt" {
			openAPIClient = openai.NewClient(openaiApiKey)
		} else {
			claudeAPIClient = anthropic.NewClient(
				anthropicApiKey,
				anthropic.WithBetaVersion(anthropic.BetaMaxTokens35Sonnet20240715))
		}
		caller := func(contextTxt, chunk string) (string, error) {
			if model == "chatgpt" {
				return callChatGPTAPIWithBackoff(openAPIClient, contextTxt, chunk)
			} else if model == "claude" {
				return callClaudeAPIWithBackoff(claudeAPIClient, contextTxt, chunk)
			}
			panic("should never get here")
		}

		var cleanedTranscript strings.Builder
		for i, chunk := range chunks {
			cleanedChunk, err := caller(contextTxt, chunk)
			if err != nil {
				return fmt.Errorf("failed to process chunk: %w", err)
			}
			cleanedTranscript.WriteString(cleanedChunk + " ")
			fmt.Printf("transcribed part %d/%d…\n", i+1, len(chunks))
		}

		if err != nil {
			return fmt.Errorf("failed to process chunk: %w", err)
		}

		cleanedTranscriptFilename := path.Join(folder, fmt.Sprintf("cleaned_transcript_%s.txt", filenameSuffix))
		if err = os.WriteFile(cleanedTranscriptFilename, []byte(cleanedTranscript.String()), 0644); err != nil {
			return fmt.Errorf("failed to write cleaned transcript: %w", err)
		}
		fmt.Printf("wrote cleaned up transcripts to %s\n", cleanedTranscriptFilename)
		return nil
	},
}

func init() {
	Command.Flags().StringP("path", "p", "", "save raw and cleaned up transcripts to path")
	Command.Flags().StringP("suffix", "s", "", "append suffix to filenames")
	Command.Flags().BoolP("raw", "r", false, "download raw transcript, don't cleanup using LLM")
	Command.Flags().VarP(enumflag.New(new(Model), "model", modelMap, enumflag.EnumCaseInsensitive), "model", "m", "use specified model: can be 'chatgpt' (default if omitted) or 'claude'")
}
